{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Agent-Environment Interface\n",
    "\n",
    "Markov Decision Processes (MDPs) provide a direct framework for learning from experience and interaction to achieve a goal, similar to reinforcement learning. To apply this framework, we need to understand some of its main components:\n",
    "\n",
    "1. **Agent**: The learner or decision maker.\n",
    "2. **Environment**: Everything that the agent interacts with.\n",
    "\n",
    "For each *action* that the agent performs in the current *state* provided by the environment, the environment returns a *reward* and the next state. This process is better described in the following figure:\n",
    "\n",
    "![Agent-Environment Interface](https://drive.google.com/uc?export=view&id=1zc5K7_nGynQULLzN6PgpAp0evoCf2Q8s)\n",
    "\n",
    " The agent and eviroment interact at each of a sequence of discrete time steps $t = 0, 1, 2, 3, \\ldots$. At each time step, the agent receives the current state $S_t \\in \\mathcal{S}$, selects an action $A_t \\in \\mathcal{A}(s)$, receives a reward $R_{t+1} \\in \\mathcal{R} \\subset \\mathbb{R}$, and transitions to the next state $S_{t+1}$. The MDP and the agent together rise to a sequence or trajectory that begins like this:\n",
    "\n",
    " $$ S_0,A_0,R_1,S_1,A_1,R_2,S_2,A_2,R_3 \\ldots $$\n",
    "\n",
    " In an finite MDP, the sets of states, actions, and rewards ($\\mathcal{S}, \\mathcal{A}, \\mathcal{R}$) are all finite. In this case, the random variables $R_t$ and $S_t$ have well-defined discrete probability distributions dependent only on the preceding state and action. That is, for particular values of these random variables, $s' \\in \\mathcal{S}$ and $r \\in \\mathcal{R}$, there is a probability of $r$ and $s'$ of these values occurring at time $t$, given that the preceding state and action:\n",
    "\n",
    " $$ p(s',r | s,a) \\doteq \\text{Pr}\\{S_t = s', R_t = r | S_{t-1} = s, A_{t-1} = a\\} $$\n",
    "\n",
    " We call to $p$ the *dynamics* of the MDP. The dynamics fully specify the environment's reaction to the agent's actions. Therefore, since $p$ specifies a probablity distribution for each choice of $s, a$, that is, that:\n",
    "\n",
    " $$ \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s',r | s,a) = 1, \\text{ for all } s \\in \\mathcal{S}, a \\in \\mathcal{A}(s) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
